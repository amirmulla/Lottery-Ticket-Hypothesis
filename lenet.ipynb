{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lenet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1I2HpbYnud-g8ANEb65AMvGeLtO9HrQa-",
      "authorship_tag": "ABX9TyOmTx5b0PLkgR3gToeJq+Y8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirmulla/Lottery-Ticket-Hypothesis/blob/main/lenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgJRyJGo5a8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746e344c-1393-4e77-a250-a9af5715ff83"
      },
      "source": [
        "###########################\n",
        "# Mount Google Drive      #\n",
        "###########################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "results_dir = '/content/drive/MyDrive/Deep Learning Project/Results'\n",
        "model_dir = '/content/drive/MyDrive/Deep Learning Project/Models'"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljVVhBMU6ve"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "import time\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import helper\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQYxzFjOYEZ2",
        "outputId": "6be4cb9c-251e-42f9-d675-dd9f5d39cafe"
      },
      "source": [
        "# define the NN architecture\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, int_range=0.5):\n",
        "        super(LeNet, self).__init__()\n",
        "        # number of hidden nodes in each layer\n",
        "        hidden_1 = 300\n",
        "        hidden_2 = 100\n",
        "        # linear layer (784 -> hidden_1)\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_1)\n",
        "        # linear layer (n_hidden -> hidden_2)\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "        # linear layer (n_hidden -> 10)\n",
        "        self.fc3 = nn.Linear(hidden_2, 10)\n",
        "        # dropout layer (p=0.2)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # activation\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        # initilize weights\n",
        "        self.init_weights_list = []\n",
        "        self.init_weights(int_range)      \n",
        "\n",
        "        # prune layers\n",
        "        self.prune_list = []\n",
        "        self.prune_list.append(self.fc1)\n",
        "        self.prune_list.append(self.fc2)\n",
        "        self.prune_list.append(self.fc3)\n",
        "\n",
        "    def init_weights(self, int_range=0.5):\n",
        "        self.fc1.weight.data.uniform_(-int_range, int_range)\n",
        "        self.fc2.weight.data.uniform_(-int_range, int_range)\n",
        "        self.fc3.weight.data.uniform_(-int_range, int_range)\n",
        "        self.init_weights_list.append(self.fc1.weight.data.clone())\n",
        "        self.init_weights_list.append(self.fc2.weight.data.clone())\n",
        "        self.init_weights_list.append(self.fc3.weight.data.clone())\n",
        "\n",
        "    def rewind_weight(self):\n",
        "        self.fc1.weight.data = self.init_weights_list[0].clone()\n",
        "        self.fc2.weight.data = self.init_weights_list[1].clone()\n",
        "        self.fc3.weight.data = self.init_weights_list[2].clone()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = self.activation(self.fc1(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = self.activation(self.fc2(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add output layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = LeNet()\n",
        "print(model)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (fc1): Linear(in_features=784, out_features=300, bias=True)\n",
            "  (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (activation): ReLU()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4dbWvQKacTT"
      },
      "source": [
        "###########################\n",
        "# Model Evaluation        #\n",
        "###########################\n",
        "\n",
        "def EvaluateModel(model, criterion, loader):\n",
        "    running_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))    \n",
        "\n",
        "    for data, target in loader:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        running_loss += loss.item()*data.size(0)\n",
        "        _, pred = torch.max(output, 1)\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        for i in range(len(target)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "    out_loss = running_loss/len(loader.sampler)\n",
        "    out_acc = np.sum(class_correct) / np.sum(class_total)\n",
        "\n",
        "    return out_loss, out_acc\n",
        "\n",
        "###########################\n",
        "# Model Training          #\n",
        "###########################\n",
        "\n",
        "def TrainAndEvaluateModel(model, model_name, epochs = 20, lr=0.001):      \n",
        "    model_res_path = results_dir + '/' + model_name + '.pt'\n",
        "    model_name = model_dir + '/' + model_name + '.pt'\n",
        "    print(model_res_path)\n",
        "    \n",
        "    model.cuda()\n",
        "\n",
        "    # Define the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_loss_list = list(0. for i in range(epochs))\n",
        "    test_loss_list = list(0. for i in range(epochs))\n",
        "    train_acc_list = list(0. for i in range(epochs))\n",
        "    test_acc_list = list(0. for i in range(epochs)) \n",
        "\n",
        "    test_loss_min = np.Inf\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Train the model\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.cuda(), labels.cuda()       \n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate the model\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_loss, train_acc = EvaluateModel(model, criterion, train_loader)\n",
        "            test_loss, test_acc = EvaluateModel(model, criterion, test_loader)             \n",
        "\n",
        "        print('Epoch: {} ({:.2f} seconds) Train Loss: {:.6f} Train Accuracy: {:.6f} Test Loss: {:.6f} Test Accuracy: {:.6f}'\n",
        "        .format(epoch+1,time.time()-t0, train_loss, train_acc, test_loss, test_acc))\n",
        "        \n",
        "        # save model if validation loss has decreased\n",
        "        if test_loss <= test_loss_min:\n",
        "          print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(test_loss_min,test_loss))\n",
        "          torch.save(model.state_dict(), model_name)\n",
        "          test_loss_min = test_loss\n",
        "\n",
        "        train_loss_list[epoch] = train_loss\n",
        "        test_loss_list[epoch] = test_loss\n",
        "        train_acc_list[epoch] = train_acc\n",
        "        test_acc_list[epoch] = test_acc\n",
        "    \n",
        "    acc_tesor_dict = {'train_loss': torch.tensor(train_loss_list),\n",
        "                      'test_loss': torch.tensor(test_loss_list),\n",
        "                      'train_acc': torch.tensor(train_acc_list),\n",
        "                      'test_acc': torch.tensor(test_acc_list)}    \n",
        "\n",
        "    print('Saving...')\n",
        "    torch.save(acc_tesor_dict, model_res_path)\n",
        "    print('Saved')"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dENbt-jEaF26"
      },
      "source": [
        "def print_sparsity(model):\n",
        "  print(\"Model sparsity: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(model.fc1.weight == 0) + torch.sum(model.fc2.weight == 0) + torch.sum(model.fc3.weight == 0))\n",
        "        / float(model.fc1.weight.nelement() + model.fc2.weight.nelement() + model.fc3.weight.nelement())))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ez5GC2jcWZW",
        "outputId": "17f0486d-bb26-4af5-df42-5bb3291f99d2"
      },
      "source": [
        "##############################\n",
        "# Run                        #\n",
        "##############################\n",
        "epochs = 3\n",
        "lr = 0.01\n",
        "batch_size = 64\n",
        "prune_amount = 0.5\n",
        "rounds = 5\n",
        "\n",
        "model = LeNet()\n",
        "\n",
        "print('Train and evaluate lenet model with Pruning:')\n",
        "\n",
        "for round in range(rounds):\n",
        "  print('Start prune round: {}'.format(round+1))\n",
        "  t0 = time.time()\n",
        "  TrainAndEvaluateModel(model, 'model_prune_lenet', epochs=epochs, lr=lr)\n",
        "  for layer in model.prune_list:\n",
        "    prune.l1_unstructured(layer, name='weight', amount=prune_amount)\n",
        "\n",
        "  print_sparsity(model)\n",
        "  #model.rewind_weight()\n",
        "  model.init_weights(int_range=0.1*round)\n",
        "\n",
        "print(f'lenet model, took {time.time()-t0: .2f} seconds')"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train and evaluate lenet model with Pruning:\n",
            "Start prune round: 1\n",
            "/content/drive/MyDrive/Deep Learning Project/Results/model_prune_lenet.pt\n",
            "Epoch: 1 (20.66 seconds) Train Loss: 0.367367 Train Accuracy: 0.905317 Test Loss: 0.376822 Test Accuracy: 0.904300\n",
            "Test loss decreased (inf --> 0.376822).  Saving model ...\n",
            "Epoch: 2 (20.83 seconds) Train Loss: 0.304711 Train Accuracy: 0.923367 Test Loss: 0.318146 Test Accuracy: 0.924100\n",
            "Test loss decreased (0.376822 --> 0.318146).  Saving model ...\n",
            "Epoch: 3 (20.86 seconds) Train Loss: 0.275076 Train Accuracy: 0.932383 Test Loss: 0.336131 Test Accuracy: 0.929000\n",
            "Saving...\n",
            "Saved\n",
            "Model global sparsity: 50.00%\n",
            "Start prune round: 2\n",
            "/content/drive/MyDrive/Deep Learning Project/Results/model_prune_lenet.pt\n",
            "Epoch: 1 (21.46 seconds) Train Loss: 0.160595 Train Accuracy: 0.960800 Test Loss: 0.223181 Test Accuracy: 0.955000\n",
            "Test loss decreased (inf --> 0.223181).  Saving model ...\n",
            "Epoch: 2 (21.63 seconds) Train Loss: 0.168927 Train Accuracy: 0.953683 Test Loss: 0.234825 Test Accuracy: 0.946500\n",
            "Epoch: 3 (21.61 seconds) Train Loss: 0.160599 Train Accuracy: 0.960217 Test Loss: 0.219986 Test Accuracy: 0.958400\n",
            "Test loss decreased (0.223181 --> 0.219986).  Saving model ...\n",
            "Saving...\n",
            "Saved\n",
            "Model global sparsity: 75.00%\n",
            "Start prune round: 3\n",
            "/content/drive/MyDrive/Deep Learning Project/Results/model_prune_lenet.pt\n",
            "Epoch: 1 (21.59 seconds) Train Loss: 0.172799 Train Accuracy: 0.956517 Test Loss: 0.256755 Test Accuracy: 0.953900\n",
            "Test loss decreased (inf --> 0.256755).  Saving model ...\n",
            "Epoch: 2 (21.83 seconds) Train Loss: 0.151519 Train Accuracy: 0.963217 Test Loss: 0.227002 Test Accuracy: 0.958000\n",
            "Test loss decreased (0.256755 --> 0.227002).  Saving model ...\n",
            "Epoch: 3 (22.03 seconds) Train Loss: 0.115652 Train Accuracy: 0.971267 Test Loss: 0.216626 Test Accuracy: 0.964000\n",
            "Test loss decreased (0.227002 --> 0.216626).  Saving model ...\n",
            "Saving...\n",
            "Saved\n",
            "Model global sparsity: 87.50%\n",
            "Start prune round: 4\n",
            "/content/drive/MyDrive/Deep Learning Project/Results/model_prune_lenet.pt\n",
            "Epoch: 1 (21.79 seconds) Train Loss: 0.240355 Train Accuracy: 0.933867 Test Loss: 0.323603 Test Accuracy: 0.928500\n",
            "Test loss decreased (inf --> 0.323603).  Saving model ...\n",
            "Epoch: 2 (21.59 seconds) Train Loss: 0.146182 Train Accuracy: 0.958617 Test Loss: 0.229427 Test Accuracy: 0.949200\n",
            "Test loss decreased (0.323603 --> 0.229427).  Saving model ...\n",
            "Epoch: 3 (21.74 seconds) Train Loss: 0.117169 Train Accuracy: 0.966183 Test Loss: 0.187790 Test Accuracy: 0.958000\n",
            "Test loss decreased (0.229427 --> 0.187790).  Saving model ...\n",
            "Saving...\n",
            "Saved\n",
            "Model global sparsity: 93.75%\n",
            "Start prune round: 5\n",
            "/content/drive/MyDrive/Deep Learning Project/Results/model_prune_lenet.pt\n",
            "Epoch: 1 (21.82 seconds) Train Loss: 0.588559 Train Accuracy: 0.771067 Test Loss: 0.619092 Test Accuracy: 0.771300\n",
            "Test loss decreased (inf --> 0.619092).  Saving model ...\n",
            "Epoch: 2 (21.63 seconds) Train Loss: 0.472423 Train Accuracy: 0.822167 Test Loss: 0.502334 Test Accuracy: 0.819200\n",
            "Test loss decreased (0.619092 --> 0.502334).  Saving model ...\n",
            "Epoch: 3 (21.72 seconds) Train Loss: 0.423361 Train Accuracy: 0.842900 Test Loss: 0.453360 Test Accuracy: 0.840700\n",
            "Test loss decreased (0.502334 --> 0.453360).  Saving model ...\n",
            "Saving...\n",
            "Saved\n",
            "Model global sparsity: 96.88%\n",
            "lenet model, took  65.22 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lunMzEvCd3Zv",
        "outputId": "841e2b09-75ef-4b90-ef16-2860d51de614"
      },
      "source": [
        "  print('Final:')\n",
        "  t0 = time.time()\n",
        "  TrainAndEvaluateModel(model, 'model_prune_lenet', epochs=epochs, lr=lr)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final:\n",
            "/content/drive/MyDrive/Deep Learning Project/Results/model_prune_lenet.pt\n",
            "Epoch: 1 (21.83 seconds) Train Loss: 1.872352 Train Accuracy: 0.296117 Test Loss: 1.871366 Test Accuracy: 0.297900\n",
            "Test loss decreased (inf --> 1.871366).  Saving model ...\n",
            "Epoch: 2 (21.69 seconds) Train Loss: 1.812236 Train Accuracy: 0.325033 Test Loss: 1.803413 Test Accuracy: 0.333000\n",
            "Test loss decreased (1.871366 --> 1.803413).  Saving model ...\n",
            "Epoch: 3 (21.89 seconds) Train Loss: 1.793870 Train Accuracy: 0.341667 Test Loss: 1.786741 Test Accuracy: 0.349700\n",
            "Test loss decreased (1.803413 --> 1.786741).  Saving model ...\n",
            "Saving...\n",
            "Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmG36WrwfSLS"
      },
      "source": [
        "######################################\n",
        "# Loading Trained model              #\n",
        "######################################\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "train_model = LeNet()\n",
        "train_model.cuda()\n",
        "model_name = model_dir + '/model_prune_lenet.pt'\n",
        "train_model.load_state_dict(torch.load(model_name))\n",
        "\n",
        "train_model.eval()\n",
        "with torch.no_grad():\n",
        "  test_loss, test_acc = EvaluateModel(train_model, criterion, test_loader)         \n",
        "  print('Test Loss: {:.6f} Test Accuracy: {:.6f}'.format(test_loss, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hIzMwvngBDD",
        "outputId": "a2250506-692b-43b6-817d-ded386120de0"
      },
      "source": [
        "#model_parameters = filter(lambda p: p.requires_grad, train_model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model.parameters()])\n",
        "print(params)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "266610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1KrGEcCcu0a",
        "outputId": "c91fa4cb-55d5-4c40-8b6c-fd4232e35694"
      },
      "source": [
        "print_sparsity(model)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model global sparsity: 96.88%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVrVXPXGeUT-"
      },
      "source": [
        "print(list(model.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}